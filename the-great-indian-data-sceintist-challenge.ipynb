{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multi class text classification using Google BERT and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['googleresearchbert', 'the-great-indian-data-scientist-hiring-challenge', 'bert-pretrained-models']\n"
     ]
    }
   ],
   "source": [
    "'''In this kernel we will implement multi class text classification using Google BERT and tensorflow'''\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import collections\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['googleresearchbert', 'the-great-indian-data-scientist-hiring-challenge', 'bert-pretrained-models']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "print(os.listdir(\"../input\"))\n",
    "import fileinput\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import datetime\n",
    "import sys\n",
    "from tqdm  import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\r\n",
      "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from bert-tensorflow) (1.12.0)\r\n",
      "Installing collected packages: bert-tensorflow\r\n",
      "Successfully installed bert-tensorflow-1.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-03 03:13:32--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\r\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.69.128, 2a00:1450:4013:c04::80\r\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.69.128|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 407727028 (389M) [application/zip]\r\n",
      "Saving to: ‘uncased_L-12_H-768_A-12.zip’\r\n",
      "\r\n",
      "uncased_L-12_H-768_ 100%[===================>] 388.84M   107MB/s    in 3.6s    \r\n",
      "\r\n",
      "2019-07-03 03:13:36 (107 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\r\n",
      "\r\n",
      "--2019-07-03 03:13:37--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 37922 (37K) [text/plain]\r\n",
      "Saving to: ‘modeling.py’\r\n",
      "\r\n",
      "modeling.py         100%[===================>]  37.03K  --.-KB/s    in 0.04s   \r\n",
      "\r\n",
      "2019-07-03 03:13:37 (909 KB/s) - ‘modeling.py’ saved [37922/37922]\r\n",
      "\r\n",
      "--2019-07-03 03:13:38--  https://raw.githubusercontent.com/google-research/bert/master/optimization.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 6258 (6.1K) [text/plain]\r\n",
      "Saving to: ‘optimization.py’\r\n",
      "\r\n",
      "optimization.py     100%[===================>]   6.11K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2019-07-03 03:13:38 (55.4 MB/s) - ‘optimization.py’ saved [6258/6258]\r\n",
      "\r\n",
      "--2019-07-03 03:13:39--  https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 34783 (34K) [text/plain]\r\n",
      "Saving to: ‘run_classifier.py’\r\n",
      "\r\n",
      "run_classifier.py   100%[===================>]  33.97K  --.-KB/s    in 0.004s  \r\n",
      "\r\n",
      "2019-07-03 03:13:39 (8.39 MB/s) - ‘run_classifier.py’ saved [34783/34783]\r\n",
      "\r\n",
      "--2019-07-03 03:13:40--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 12257 (12K) [text/plain]\r\n",
      "Saving to: ‘tokenization.py’\r\n",
      "\r\n",
      "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2019-07-03 03:13:40 (85.1 MB/s) - ‘tokenization.py’ saved [12257/12257]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'model_folder'\n",
    "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Model output directory: model_folder/outputs\n",
      ">>  BERT pretrained directory: model_folder/uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "#import tokenization\n",
    "#import modeling\n",
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
    "BERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\n",
    "OUTPUT_DIR = f'{folder}/outputs'\n",
    "print(f'>> Model output directory: {OUTPUT_DIR}')\n",
    "print(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')\n",
    "BERT_VOCAB= '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "BERT_INIT_CHKPNT = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "BERT_CONFIG = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization.validate_case_matches_checkpoint(True,BERT_INIT_CHKPNT)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=BERT_VOCAB, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: DeprecationWarning: invalid escape sequence \\s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                     # For mathematical calculations\n",
    "import seaborn as sns                  # For data visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sn                   # For plotting graphs\n",
    "import re\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "import warnings                        # To ignore any warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    " \n",
    "le = LabelEncoder()\n",
    " \n",
    "df = pd.read_csv(\"../input/the-great-indian-data-scientist-hiring-challenge/Train.csv\")\n",
    "\n",
    "df['Item_Description'] = df['Item_Description'].apply(lambda x : re.sub(r'\\b[A-Z]+\\b', '',re.sub('\\s+',' ',re.sub(r'[^a-zA-Z]', ' ',x))))\n",
    "df['Item_Description'] = df['Item_Description'].apply(lambda x : x.lower())\n",
    "identity_columns = ['CLASS-784', 'CLASS-95', 'CLASS-51', 'CLASS-559', 'CLASS-489', 'CLASS-913', 'CLASS-368', 'CLASS-816', 'CLASS-629', 'CLASS-177', 'CLASS-123', 'CLASS-671', 'CLASS-804', 'CLASS-453', 'CLASS-1042', 'CLASS-49', 'CLASS-947', 'CLASS-110', 'CLASS-278', 'CLASS-522', 'CLASS-606', 'CLASS-651', 'CLASS-765', 'CLASS-953', 'CLASS-839', 'CLASS-668', 'CLASS-758', 'CLASS-942', 'CLASS-764', 'CLASS-50', 'CLASS-75', 'CLASS-74', 'CLASS-783', 'CLASS-323', 'CLASS-322', 'CLASS-720', 'CLASS-230', 'CLASS-571'] \n",
    "for key in identity_columns:\n",
    "    df.Product_Category[df.Product_Category==key] = identity_columns.index(key)+1\n",
    "print(len(identity_columns))\n",
    "\n",
    "df2 = pd.DataFrame({'text':df['Item_Description'].replace(r'\\n',' ',regex=True),\n",
    "            'label':LabelEncoder().fit_transform(df['Product_Category'].replace(r' ','',regex=True)),\n",
    "            })\n",
    "\n",
    "# Creating train and val dataframes according to BERT\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2[\"text\"].values, df2[\"label\"].values, test_size=0.2, random_state=42)\n",
    "X_train, y_train = df2[\"text\"].values, df2[\"label\"].values\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Creating test dataframe according to BERT\n",
    "testpd = pd.read_csv(\"../input/the-great-indian-data-scientist-hiring-challenge/Test.csv\")\n",
    "testpd['Item_Description'] = testpd['Item_Description'].apply(lambda x : re.sub(r'\\b[A-Z]+\\b', '',re.sub('\\s+',' ',re.sub(r'[^a-zA-Z]', ' ',x))))\n",
    "testpd['Item_Description'] = testpd['Item_Description'].apply(lambda x : x.lower())\n",
    "test = pd.DataFrame({'text':testpd['Item_Description'].replace(r'\\n',' ',regex=True)})\n",
    "test = test[\"text\"].values\n",
    "\n",
    "# Saving dataframes to .tsv format as required by BERT\n",
    "#X_train.to_csv('train.tsv', sep='\\t', index=False, header=False)\n",
    "#X_test.to_csv('dev.tsv', sep='\\t', index=False, header=False)\n",
    "#test.to_csv('test.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['   apr store management real estate real estate services real estate general search appraisal realtor commission ',\n",
       "       '   nov store management real estate real estate services real estate general search appraisal realtor commission ',\n",
       "       '  nov store management real estate real estate services real estate general search appraisal realtor commission ',\n",
       "       '     aug store management real estate real estate services real estate general search appraisal realtor commission ',\n",
       "       '     mar store management real estate real estate services real estate general search appraisal realtor commission '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5] # check training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_examples(lines, set_type, labels=None):\n",
    "#Generate data for the BERT model\n",
    "    guid=f'{set_type}'\n",
    "    examples = []\n",
    "    if set_type == 'train':\n",
    "        for line, label in zip(lines, labels):\n",
    "            \n",
    "            text_a = line\n",
    "            label = str(label)\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    else:\n",
    "        for line in lines:\n",
    "            \n",
    "            text_a = line\n",
    "            label = '0'\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples\n",
    "\n",
    "# Model Hyper Parameters\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_SEQ_LENGTH = 100\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
    "# each checpoint weights about 1,5gb\n",
    "ITERATIONS_PER_LOOP = 100\n",
    "NUM_TPU_CORES = 8\n",
    "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
    "\n",
    "label_list = [str(num) for num in range(38)]\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
    "train_examples = create_examples(X_train, 'train', labels=y_train)\n",
    "\n",
    "tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
    "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "model_fn = run_classifier.model_fn_builder(\n",
    "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
    "    num_labels=len(label_list),\n",
    "    init_checkpoint=INIT_CHECKPOINT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
    "    use_one_hot_embeddings=True)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using traing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      ">> Started training at 2019-07-03 03:13:54.365906 \n",
      "  Num examples = 5719\n",
      "  Batch size = 16\n",
      ">> Finished training at 2019-07-03 03:19:15.209332\n"
     ]
    }
   ],
   "source": [
    "print('Please wait...')\n",
    "train_features = run_classifier.convert_examples_to_features(\n",
    "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "print('>> Started training at {} '.format(datetime.now()))\n",
    "print('  Num examples = {}'.format(len(train_examples)))\n",
    "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "train_input_fn = run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('>> Finished training at {}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_id)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    print(params)\n",
    "    batch_size = 500\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_examples = create_examples(X_test, 'test')\n",
    "predict_features = run_classifier.convert_examples_to_features(\n",
    "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "predict_input_fn = input_fn_builder(\n",
    "    features=predict_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for prediction in result:\n",
    "      preds.append(np.argmax(prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 30, 3, 3, 33, 29, 28, 33, 30, 3, 30, 30, 3, 33, 9, 8, 9, 30, 9, 3, 3, 18, 9, 3, 12, 30, 3, 33, 3, 3, 8, 3, 5, 2, 28, 3, 30, 3, 19, 3, 30, 30, 33, 21, 6, 33, 18, 33, 3, 33, 8, 30, 3, 3, 30, 30, 30, 33, 29, 3, 28, 3, 3, 3, 28, 28, 19, 2, 29, 30, 28, 3, 9, 9, 33, 30, 3, 30, 3, 30, 3, 30, 33, 3, 33, 2, 30, 3, 30, 27, 33, 30, 3, 30, 28, 27, 3, 12, 9, 3, 3, 3, 33, 33, 9, 33, 3, 33, 9, 28, 2, 3, 35, 3, 30, 30, 3, 30, 3, 33, 33, 9, 30, 28, 35, 9, 33, 33, 21, 3, 9, 33, 30, 19, 19, 30, 30, 2, 8, 30, 30, 28, 3, 27, 3, 29, 29, 33, 9, 28, 18, 33, 2, 3, 3, 3, 18, 9, 6, 2, 21, 6, 33, 33, 30, 30, 28, 3, 3, 33, 33, 6, 29, 9, 30, 3, 18, 28, 2, 2, 3, 19, 2, 21, 19, 2, 3, 9, 30, 3, 30, 30, 30, 35, 19, 33, 9, 28, 30, 14, 29, 6, 3, 28, 8, 3, 33, 2, 29, 33, 30, 29, 27, 33, 29, 9, 2, 29, 5, 3, 3, 30, 19, 27, 3, 27, 3, 29, 29, 3, 29, 3, 3, 3, 30, 19, 19, 29, 30, 3, 19, 29, 3, 12, 3, 33, 6, 3, 29, 30, 9, 2, 9, 3, 30, 30, 6, 28, 28, 8, 29, 28, 33, 28, 5, 33, 30, 30, 3, 3, 3, 12, 3, 3, 2, 29, 3, 33, 30, 3, 3, 3, 33, 19, 2, 3, 3, 33, 2, 2, 3, 3, 2, 2, 30, 3, 3, 2, 30, 2, 33, 3, 3, 30, 3, 30, 30, 21, 30, 33, 3, 3, 3, 9, 3, 29, 30, 30, 9, 6, 6, 30, 3, 3, 3, 2, 28, 2, 19, 30, 30, 33, 33, 3, 2, 3, 3, 30, 3, 2, 30, 3, 30, 3, 30, 30, 30, 33, 29, 6, 30, 19, 33, 30, 6, 30, 9, 28, 33, 30, 30, 33, 30, 3, 9, 33, 9, 33, 33, 3, 28, 30, 30, 30, 3, 30, 3, 30, 30, 2, 29, 3, 33, 19, 3, 2, 33, 12, 33, 27, 27, 3, 3, 9, 3, 19, 29, 3, 30, 30, 33, 3, 27, 19, 29, 33, 9, 5, 30, 27, 8, 3, 19, 3, 33, 6, 9, 9, 2, 30, 27, 3, 3, 6, 29, 33, 35, 19, 3, 29, 6, 27, 29, 2, 9, 3, 2, 30, 19, 27, 3, 3, 3, 3, 30, 27, 33, 30, 30, 30, 33, 3, 3, 33, 3, 9, 3, 28, 30, 3, 3, 28, 6, 29, 3, 30, 30, 3, 3, 33, 30, 3, 30, 28, 2, 9, 33, 33, 30, 33, 29, 30, 30, 29, 3, 30, 33, 33, 30, 6, 35, 30, 28, 33, 33, 6, 28, 21, 33, 30, 29, 9, 3, 27, 19, 2, 27, 33, 2, 30, 3, 30, 28, 3, 33, 3, 33, 3, 9, 29, 27, 29, 30, 33, 33, 19, 2, 3, 30, 3, 30, 30, 3, 9, 8, 6, 28, 27, 30, 12, 2, 2, 3, 3, 29, 30, 3, 3, 30, 19, 33, 33, 2, 2, 30, 9, 3, 28, 3, 29, 8, 28, 3, 28, 30, 9, 29, 3, 9, 30, 33, 3, 9, 2, 9, 30, 5, 3, 19, 19, 21, 30, 3, 6, 6, 3, 33, 3, 33, 30, 29, 28, 27, 33, 9, 3, 33, 33, 2, 27, 3, 2, 3, 19, 3, 33, 2, 3, 3, 3, 30, 30, 9, 33, 9, 3, 33, 33, 3, 33, 9, 33, 2, 3, 19, 9, 33, 3, 3, 29, 3, 30, 3, 21, 33, 3, 3, 3, 9, 30, 29, 30, 33, 30, 8, 29, 27, 30, 18, 33, 3, 27, 2, 33, 6, 33, 30, 2, 6, 30, 6, 3, 2, 3, 9, 29, 29, 30, 30, 3, 3, 3, 9, 33, 8, 29, 2, 27, 33, 5, 28, 3, 30, 27, 3, 3, 28, 30, 29, 27, 2, 6, 9, 3, 9, 3, 9, 33, 18, 28, 3, 18, 6, 33, 3, 19, 28, 9, 2, 3, 3, 28, 30, 2, 2, 9, 3, 3, 2, 19, 9, 9, 2, 30, 3, 2, 28, 30, 33, 9, 29, 9, 33, 3, 27, 12, 28, 3, 3, 3, 3, 19, 6, 2, 19, 9, 33, 2, 30, 2, 9, 33, 3, 3, 33, 3, 3, 3, 27, 33, 30, 33, 2, 19, 5, 3, 30, 19, 19, 3, 3, 3, 3, 30, 33, 33, 33, 33, 30, 33, 33, 33, 3, 8, 3, 3, 2, 3, 3, 3, 3, 3, 9, 3, 33, 30, 3, 6, 30, 29, 8, 30, 33, 3, 29, 2, 8, 2, 3, 3, 3, 28, 19, 29, 3, 30, 33, 3, 2, 3, 35, 3, 30, 3, 2, 30, 33, 9, 3, 19, 33, 30, 33, 30, 8, 33, 27, 19, 3, 30, 9, 21, 2, 3, 3, 6, 30, 3, 3, 29, 9, 3, 3, 3, 3, 30, 30, 30, 8, 3, 2, 2, 30, 3, 28, 3, 3, 3, 30, 30, 3, 9, 3, 3, 3, 3, 3, 29, 2, 3, 35, 33, 33, 30, 2, 9, 3, 3, 35, 3, 3, 19, 30, 30, 33, 27, 3, 35, 2, 3, 33, 30, 30, 30, 33, 3, 30, 29, 3, 33, 30, 19, 3, 2, 3, 2, 9, 19, 6, 35, 30, 33, 30, 33, 35, 9, 3, 2, 33, 6, 3, 9, 30, 33, 3, 3, 30, 2, 29, 28, 2, 3, 33, 28, 3, 8, 29, 19, 19, 28, 33, 29, 33, 27, 33, 3, 8, 30, 30, 30, 3, 9, 19, 30, 33, 6, 33, 29, 30, 33, 30, 8, 33, 2, 2, 19, 9, 30, 9, 30, 12, 3, 9, 27, 5, 33, 21, 30, 3, 27, 30, 3, 33, 21, 29, 30, 29, 29, 3, 30, 3, 3, 30, 3, 33, 33, 29, 3, 9, 19, 8, 28, 2, 3, 5, 9, 30, 19, 33, 33, 28, 3, 33, 33, 29, 30, 35, 19, 30, 30, 9, 2, 30, 3, 30, 3, 3, 3, 33, 9, 30, 6, 3, 33, 33, 3, 3, 3, 30, 33, 33, 3, 35, 3, 2, 33, 3, 29, 33, 28, 33, 3, 3, 28, 3, 33, 3, 28, 3, 9, 6, 3, 3, 29, 33, 9, 19, 30, 2, 14, 3, 27, 29, 3, 30, 2, 30, 3, 33, 29, 29, 33, 30, 30, 30, 35, 3, 3, 30, 6, 3, 9, 9, 30, 30, 9, 33, 3, 3, 3, 29, 3, 28, 3, 3, 19, 3, 28, 33, 8, 3, 33, 5, 33, 28, 30, 33, 30, 30, 33, 30, 33, 29, 19, 3, 30, 33, 29, 33, 9, 30, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BERT is: 0.9012237762237763\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of BERT is:\",accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy obtained is about 90.12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       1.00      1.00      1.00        84\n",
      "           3       1.00      1.00      1.00       302\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.53      1.00      0.69        18\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       1.00      1.00      1.00        21\n",
      "           9       1.00      1.00      1.00        82\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.00      0.00      0.00        11\n",
      "          12       1.00      1.00      1.00         8\n",
      "          13       0.00      0.00      0.00         5\n",
      "          14       0.50      0.11      0.18         9\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         4\n",
      "          18       0.00      0.00      0.00        12\n",
      "          19       0.52      1.00      0.68        26\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       1.00      1.00      1.00        11\n",
      "          22       0.00      0.00      0.00         6\n",
      "          23       0.00      0.00      0.00         2\n",
      "          24       0.00      0.00      0.00         6\n",
      "          25       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.79      1.00      0.88        26\n",
      "          28       0.89      1.00      0.94        47\n",
      "          29       0.63      1.00      0.77        43\n",
      "          30       0.97      1.00      0.98       193\n",
      "          31       0.00      0.00      0.00         6\n",
      "          33       0.88      1.00      0.94       145\n",
      "          34       0.00      0.00      0.00        20\n",
      "          35       1.00      1.00      1.00        14\n",
      "          36       0.00      0.00      0.00         6\n",
      "          37       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.90      1144\n",
      "   macro avg       0.37      0.41      0.38      1144\n",
      "weighted avg       0.84      0.90      0.87      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess actual test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpd = pd.read_csv(\"../input/the-great-indian-data-scientist-hiring-challenge/Test.csv\")\n",
    "testpd['Item_Description'] = testpd['Item_Description'].apply(lambda x : re.sub(r'\\b[A-Z]+\\b', '',re.sub('\\s+',' ',re.sub(r'[^a-zA-Z]', ' ',x))))\n",
    "testpd['Item_Description'] = testpd['Item_Description'].apply(lambda x : x.lower())\n",
    "testln = pd.DataFrame({'guid':testpd['Inv_Id'],\n",
    "    'text':testpd['Item_Description'].replace(r'\\n',' ',regex=True)})\n",
    "testl = testln[\"text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions on actual test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = create_examples(testl, 'test')\n",
    "predict_features1 = run_classifier.convert_examples_to_features(\n",
    "    predict_test, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "predict_input_fn1 = input_fn_builder(\n",
    "    features=predict_features1,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result1 = estimator.predict(input_fn=predict_input_fn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "preds1 = []\n",
    "for prediction in result1 :\n",
    "    #print(prediction)\n",
    "    preds1.append(np.argmax(prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 19, 19, 19, 19, 19, 19, 29, 29, 5, 6, 14, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 6, 19, 19, 19, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 6, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 12, 12, 12, 12, 29, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 12, 12, 12, 12, 12, 12, 12, 14, 29, 29, 6, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 5, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 21, 21, 21, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 19, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 29, 29, 29, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 6, 6, 6, 6, 5, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 5, 5, 28, 28, 28, 28, 28, 28, 28, 5, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 5, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 5, 28, 28, 28, 28, 28, 28, 28, 28, 28, 5, 5, 28, 28, 28, 28, 28, 28, 5, 28, 28, 5, 5, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 5, 5, 28, 28, 28, 28, 28, 28, 28, 28, 28, 5, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 6, 29, 29, 29, 29, 29, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 29, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 6, 6, 6, 35, 35, 35, 35, 35, 6, 35, 6, 6, 35, 35, 6, 35, 35, 18, 5, 19, 27, 19, 19, 19, 19, 19, 18, 18, 19, 19, 19, 19, 18, 19, 18, 18, 18, 18, 18, 18]\n"
     ]
    }
   ],
   "source": [
    "print(preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(predictions):\n",
    "    probabilities = []\n",
    "    for (i, prediction) in enumerate(predictions):\n",
    "        preds = prediction\n",
    "        probabilities.append(preds)\n",
    "        #print(preds)\n",
    "    dff = pd.DataFrame(probabilities)\n",
    "    dff.head()\n",
    "    \n",
    "    #dff.columns = identity_columns\n",
    "    \n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_df = create_output(preds1)\n",
    "predslab=[identity_columns[x] for x in preds1]\n",
    "testln[\"Product_Category\"]=predslab\n",
    "merged_df =  testln\n",
    "submission = merged_df.drop(['text'], axis=1)\n",
    "submission.to_csv(\"sample_submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>Product_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>CLASS-522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>CLASS-522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>CLASS-522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>CLASS-522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>CLASS-522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   guid Product_Category\n",
       "0     6        CLASS-522\n",
       "1    12        CLASS-522\n",
       "2    14        CLASS-522\n",
       "3    18        CLASS-522\n",
       "4    19        CLASS-522"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have completed predicting product class and stored the results to a csv file.\n",
    "Hope you enjoyed the kernel.\n",
    "Feel free to use this kernel as a base doing your own multi class text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
